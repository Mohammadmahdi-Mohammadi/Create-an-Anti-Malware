# Create-an-Anti-Malware
Malware programming is one of the most adversarial ways to earn money, but it was popular in
2014(based on virustotal.com report). As you guessed, anti-malware programming had the same
behavior to make money with. based on two feature sets, can detect malware; one of them is based on
static behavior (which accessible before the running it; like file signature or reserved memory allocation)
and the other one is dynamic behavior (which accessible after the running it; like network
communication or OS`s API call) You are given the custom dataset that consists of OS API call
behavior; follow the below parts and try to present an ant-malware model. You are allowed to use
libraries. (the label is “OUTPUT”)

a) Which kind of error is more important in anti-malware applications (both security and user
experience sides)? False positive rate or False Negative rate? why?

b) What is stratified splitting, and when should it be used? Using it, drop half the dataset (to reduce
the computational cost; you can keep it if you have appropriate resources) and split it to train/test
with the ratio of 8:2.

c) Use the Bagging classifier to detect malware. First, use the K-NN for the base estimator, and
second, use the decision tree. Try to catch the best result for each one(by fine-tuning the
parameters) and compare the results. (Do not forget that set the count of models in ensembles is
essential)

d) Next, use the Adaboost to detect malware. First, use the SVC for the base estimator, and second,
use the decision tree. Try to catch the best result for each one (by fine-tuning the parameters) and
compare the results. (Do not forget that set the count of models in ensembles is essential)

e) Using the replacement and stratified splitting, create 100 datasets, each with 5% of the original
dataset. (Create from the training set)

f) Now, train 55 decision trees, 20 SVCs, 15 multinomial naive Bayes classifiers, five K-NNs, and
five logistic regressions. For each of mentioned models, use one of the generated datasets.
(remember to use weak classifiers)

g) Now, we want you to generate a binary dataset based on the results of weak models and use it to
predict the final target (like a weighted Bagging in which weights are obtained by another model).
For this aim, pass each sample(both train and test) to 100 above models and store their prediction
as a 100-D vector to the related sample. In the end, you should have a table that consists of 101
columns (100 models + label), and its rows are equal to the count of the dataset. (do not combine
the test and train sets)

h) In the end, train an SVC and a logistic regression classifier with the obtained binary dataset (use
the train set for training and test set for testing). Compare the results with each other and previous
parts. which one is better, in your opinion? why?

In each model you trained, report normalized confusion matric with recall, accuracy, and ROC plot.
